{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This train type uses collate function, batch is padded with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:16.254380Z",
     "iopub.status.busy": "2023-03-12T09:30:16.253602Z",
     "iopub.status.idle": "2023-03-12T09:30:43.839954Z",
     "shell.execute_reply": "2023-03-12T09:30:43.838091Z",
     "shell.execute_reply.started": "2023-03-12T09:30:16.254316Z"
    }
   },
   "outputs": [],
   "source": [
    "# # !pip3 install -q -U huggingface-hub==0.13.0\n",
    "# !pip3 install -q -U graphviz==0.20.1\n",
    "# !pip3 install -q torchview==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:43.843767Z",
     "iopub.status.busy": "2023-03-12T09:30:43.843299Z",
     "iopub.status.idle": "2023-03-12T09:30:43.851301Z",
     "shell.execute_reply": "2023-03-12T09:30:43.849278Z",
     "shell.execute_reply.started": "2023-03-12T09:30:43.843723Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path = ['../input/google-ai-utils'] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:43.855373Z",
     "iopub.status.busy": "2023-03-12T09:30:43.854004Z",
     "iopub.status.idle": "2023-03-12T09:30:45.218958Z",
     "shell.execute_reply": "2023-03-12T09:30:45.217760Z",
     "shell.execute_reply.started": "2023-03-12T09:30:43.855309Z"
    }
   },
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# from torchview import draw_graph\n",
    "# graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:45.223178Z",
     "iopub.status.busy": "2023-03-12T09:30:45.221928Z",
     "iopub.status.idle": "2023-03-12T09:30:45.807060Z",
     "shell.execute_reply": "2023-03-12T09:30:45.805436Z",
     "shell.execute_reply.started": "2023-03-12T09:30:45.223127Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:45.809338Z",
     "iopub.status.busy": "2023-03-12T09:30:45.808893Z",
     "iopub.status.idle": "2023-03-12T09:30:56.518699Z",
     "shell.execute_reply": "2023-03-12T09:30:56.517142Z",
     "shell.execute_reply.started": "2023-03-12T09:30:45.809298Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "# from utils import create_label\n",
    "import numpy as np\n",
    "\n",
    "# coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time, gc, argparse, shutil\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from transformers import get_constant_schedule_with_warmup, AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForMaskedLM\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.521805Z",
     "iopub.status.busy": "2023-03-12T09:30:56.520668Z",
     "iopub.status.idle": "2023-03-12T09:30:56.535814Z",
     "shell.execute_reply": "2023-03-12T09:30:56.534696Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.521759Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch')\n",
    "parser.add_argument('--out_dir', default='', type=str, help='destination where trained network should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for training')\n",
    "parser.add_argument('--model_name', default='deberta-v3-base', type=str)\n",
    "parser.add_argument('--base_epoch', default=3, type=int, help='number of total epochs to run')#30\n",
    "parser.add_argument('--batch_size', default=2, type=int, help='train mini-batch size')# 8\n",
    "parser.add_argument('--workers', default=2, type=int, help='number of data loading workers (default: 4)')\n",
    "\n",
    "# start lr=1849e-7 and peak lr=1849e-10\n",
    "parser.add_argument('--lr', default=1849e-10, type=float, help='learning rate')\n",
    "parser.add_argument('--eta_min', default=1800e-10, type=float, help='learning rate eta_min') \n",
    "\n",
    "# without n_accumulate=5 out of memory error even at self.seq_length=128.\n",
    "parser.add_argument('--n_accumulate', default=5, type=int) \n",
    "\n",
    "parser.add_argument('--max_grad_norm', default=-1, type=float) # 1.0\n",
    "parser.add_argument('--weight_decay', default=0.0, type=float)\n",
    "parser.add_argument('--folds', default='', type=str)\n",
    "parser.add_argument('--pre_epoch', default=15, type=int)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.537940Z",
     "iopub.status.busy": "2023-03-12T09:30:56.537541Z",
     "iopub.status.idle": "2023-03-12T09:30:56.568746Z",
     "shell.execute_reply": "2023-03-12T09:30:56.567562Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.537903Z"
    }
   },
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self):\n",
    "        # data\n",
    "        self.result_dir = './models/' # ./user_data/\n",
    "        self.result_dir_1 = './kaggle/'        \n",
    "        self.data_dir = './'\n",
    "        self.model_dir = './'\n",
    "        self.k_folds = 5\n",
    "        self.random_seed = 27\n",
    "        self.seq_length = 2048 # 512; sum of \"cell length\" for a particular id.\n",
    "        self.cell_max_length = 128 # max words in a cell.\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        # model\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.gpu = 0\n",
    "        self.print_freq = 500\n",
    "        self.weight_decay = 0\n",
    "        self.optim = 'Adam'\n",
    "\n",
    "    def get(self, name):\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def set(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(['%s:%s' % item for item in self.__dict__.items()])\n",
    "    \n",
    "parameter = Parameter()\n",
    "parameter.set(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.571214Z",
     "iopub.status.busy": "2023-03-12T09:30:56.570489Z",
     "iopub.status.idle": "2023-03-12T09:30:56.590833Z",
     "shell.execute_reply": "2023-03-12T09:30:56.589344Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.571096Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "#     if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    #         torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False        \n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.593471Z",
     "iopub.status.busy": "2023-03-12T09:30:56.592743Z",
     "iopub.status.idle": "2023-03-12T09:30:56.624443Z",
     "shell.execute_reply": "2023-03-12T09:30:56.623496Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.593432Z"
    }
   },
   "outputs": [],
   "source": [
    "class MarkdownDataset(Dataset):\n",
    "\n",
    "    def __init__(self, meta_data: pd.DataFrame, tokenizer, fold: int = -1, mode='train', parameter=None):\n",
    "        self.meta_data = meta_data.copy()\n",
    "        if mode == 'train':\n",
    "            pass\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] != fold].copy()\n",
    "#             self.meta_data = self.meta_data.iloc[:60000]\n",
    "            self.meta_data.reset_index(drop=True, inplace=True)\n",
    "        elif mode == 'valid':\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] == fold].copy()\n",
    "#             self.meta_data = self.meta_data[self.meta_data['id'].isin(self.meta_data['id'].values[:1000])]\n",
    "            self.meta_data.reset_index(drop=True, inplace=True)\n",
    "        elif mode == 'test':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(mode)\n",
    "\n",
    "        if tokenizer.sep_token != '[SEP]':\n",
    "            self.meta_data['source'] = self.meta_data['source'].apply(\n",
    "                lambda x: [\n",
    "                    y.replace(tokenizer.sep_token, '').replace(tokenizer.cls_token, '').replace(tokenizer.pad_token, '')\n",
    "                    for y in x])\n",
    "        self.parameter = parameter\n",
    "        self.seq_length = self.parameter.seq_length\n",
    "        self.source = self.meta_data['source'].values\n",
    "        self.cell_type = self.meta_data['cell_type'].values\n",
    "        # self.cell_id = self.meta_data['cell_id'].values\n",
    "        self.rank = self.meta_data['rank'].values\n",
    "        # self.dense_features = self.meta_data[['cell_count','markdown_count', 'code_count']].values\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = self.source[index]\n",
    "            # (no data cleaning) source =>\n",
    "            # array([' this python 3 environment .... the current session', ..., ' на обучении на один признак больше чем на тесте '], dtype=object)\n",
    "            \n",
    "            # [len(x) for x in source] => \n",
    "            # [845, 491, 47, 40, 555, 154, 54, 15, 141, 51, 74, 35, ..., 62, 34, 39, 49]            \n",
    "        \n",
    "            # max([len(x) for x in source]), min([len(x) for x in source]) => 3722, 10\n",
    "        cell_type = self.cell_type[index]\n",
    "            # cell_type =>\n",
    "            # array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ....,  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])        \n",
    "        \n",
    "        rank = self.rank[index]\n",
    "            # rank => array([0.01724138, 0.05172414, 0.0862069 , 0.12068966, 0.15517241, ..., 0.93103448, 0.62068966, 0.4137931 ]) \n",
    "    \n",
    "        # dense_features = 1#self.dense_features[index]\n",
    "        # if self.mode == 'train':\n",
    "        #     range_tmp1 = [ i for i in range(len(cell_type)) if cell_type[i]==0]\n",
    "        #     range_tmp2 = [ i for i in range(len(cell_type)) if cell_type[i]==1]\n",
    "        #     np.random.shuffle(range_tmp2)\n",
    "        #     source = [source[i] for i in range_tmp1 + range_tmp2]\n",
    "        #     rank = [rank[i] for i in range_tmp1 + range_tmp2]\n",
    "        #     rank2 = [rank2[i] for i in range_tmp1 + range_tmp2]\n",
    "\n",
    "        cell_inputs = self.tokenizer(#.batch_encode_plus\n",
    "            source.tolist(),\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.parameter.cell_max_length,\n",
    "            # padding=\"max_length\",\n",
    "            return_attention_mask=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "            # cell_inputs =>\n",
    "            # {'input_ids': [[291, 24233, 404, ..., 2, 763, 5514, 4219], ..., [15003, 42120, 69819, ..., 15003, 65639, 109249]],\n",
    "            # 'token_type_ids': [[0, 0, 0, ..., 0, 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0]]}\n",
    "\n",
    "            # [len(x) for x in cell_inputs['input_ids']] => [128, 105, 7, 10, 127, 28, 12, 4, 26, 12, 23, 9, ... , 18]        \n",
    "            # max([len(x) for x in cell_inputs['input_ids']]), min([len(x) for x in cell_inputs['input_ids']]) => 128, 4        \n",
    "        \n",
    "        seq, seq_mask, target_mask, target = self.max_length_rule_base(cell_inputs['input_ids'], cell_type, rank)\n",
    "        \n",
    "\n",
    "        # if self.mode == 'train':\n",
    "        #     attention_mask, target = self.random_mask(attention_mask, target)\n",
    "\n",
    "        return seq, seq_mask, target_mask, target\n",
    "        # return encoded['input_ids'][0], encoded['attention_mask'][0], np.array(target, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)\n",
    "\n",
    "    def max_length_rule_base(self, cell_inputs, cell_type, rank):\n",
    "        init_length = [len(x) for x in cell_inputs]\n",
    "            # init_length => [128, 105, 7, ..., 13, 16, 18]\n",
    "            # self.seq_length => 512\n",
    "            # len(init_length) => 58\n",
    "        total_max_length = self.seq_length - len(init_length)\n",
    "            # total_max_length => 454\n",
    "        min_length = total_max_length // len(init_length)\n",
    "            # min_length => 7\n",
    "        cell_length = self.search_length(init_length, min_length, total_max_length, len(init_length))\n",
    "            # cell_length => [11, 11, 7, ..., 7, 7, 7]             \n",
    "            # len(cell_length) => 58\n",
    "        \n",
    "        seq = []\n",
    "        for i in range(len(cell_length)):\n",
    "            if cell_type[i] == 0:\n",
    "                seq.append(self.tokenizer.cls_token_id)\n",
    "                # self.tokenizer.cls_token_id => 1 \n",
    "            else:\n",
    "                seq.append(self.tokenizer.sep_token_id)\n",
    "                # self.tokenizer.sep_token_id => 2\n",
    "            if cell_length[i] > 0:\n",
    "                seq.extend(cell_inputs[i][:cell_length[i]])\n",
    "\n",
    "            # cell_inputs[i] => [291, 24233, 404, ..., 2, 763, 5514, 4219]\n",
    "            # seq => [1, 291, 24233, 404, 1192, 699, 275, 386, 2136, 7027, 7296, 2627] \n",
    "            \n",
    "        # seq => [1, 291, 24233, 404, 1192, 699, 275, 386, 2136, 7027, 7296, 2627, 1, 6306, ..., 2, 15003, ..., 88580]\n",
    "        \n",
    "        # seq_mask or attention_mask tells model which position should it attend in seq. 1 -> attend and 0 -> not attend.    \n",
    "        if len(seq) > self.seq_length:\n",
    "            seq_mask = [1] * self.seq_length\n",
    "            seq = seq[:self.seq_length]\n",
    "        else: seq_mask = [1] * len(seq) \n",
    "            \n",
    "        # seq_mask is attention_mask.\n",
    "        seq, seq_mask = np.array(seq, dtype=np.int32), np.array(seq_mask, dtype=np.int32)\n",
    "        target_mask = np.where((seq == self.tokenizer.cls_token_id) | (seq == self.tokenizer.sep_token_id), 1, 0) \n",
    "            # target_mask => [1 0 0 0 0 0 0 0 0 0 0 0 1 ..... 0 0 0 0]\n",
    "            \n",
    "        target = np.zeros(len(seq), dtype=np.float32)\n",
    "        \n",
    "        # return indices where self.tokenizer.cls_token_id or self.tokenizer.sep_token_id is present\n",
    "        tmp = np.where((seq == self.tokenizer.cls_token_id) | (seq == self.tokenizer.sep_token_id))\n",
    "            # tmp => (array([  0,  12,  24,  32,  43,  55,  67,  79,  ..., 479, 487, 495, 503]),)        \n",
    "        target[tmp] = rank\n",
    "            # target => [0.01724138 0.         0.         0.    ...      0.        ]\n",
    "            \n",
    "#         sample_weight = np.zeros(len(seq), dtype=np.float32)\n",
    "#         sample_weight = np.where(seq == self.tokenizer.cls_token_id, 0.33, sample_weight)\n",
    "#         sample_weight = np.where(seq == self.tokenizer.sep_token_id, 1.0, sample_weight)\n",
    "#         dense_features = np.zeros(self.seq_length, dtype=np.float32)\n",
    "#         dense_features[tmp] = rank2\n",
    "\n",
    "        return seq, seq_mask, target_mask, target\n",
    "\n",
    "    # Static method objects provide a way of defeating the transformation of function objects to method objects.\n",
    "    # A static method does not receive an implicit first argument. we can even call it from an instance. \n",
    "    # A class method receives the class as implicit first argument, just like an instance method receives the instance.\n",
    "    # The insertion of @staticmethod before a method definition, then, stops an instance from sending itself as an argument.\n",
    "    @staticmethod\n",
    "    def search_length(init_length, min_length, total_max_length, cell_count, step=5, max_search_count=50):# step=4\n",
    "            # init_length => [128, 105, 7, ..., 13, 16, 18] \n",
    "            # np.sum(init_length) => 1726\n",
    "            # min_length, total_max_length, cell_count => 7, 454, 58\n",
    "            # len(init_length) is cell_count.\n",
    "        if np.sum(init_length) <= total_max_length:\n",
    "            return init_length\n",
    "\n",
    "        res = [min(init_length[i], min_length) for i in range(cell_count)]\n",
    "            # res => [7, 7, 7, 7, 7, 7, 7, 4, ...., 7, 7, 7]\n",
    "        for s_i in range(max_search_count):\n",
    "            tmp = [min(init_length[i], res[i] + step) for i in range(cell_count)]# step=4\n",
    "                # tmp => [11, 11, 7, 10, 11, 11, 11, 4, ...., 11, 11, 11]\n",
    "                # np.sum(tmp) => 589\n",
    "            if np.sum(tmp) < total_max_length:\n",
    "                res = tmp\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        for s_i in range(cell_count):\n",
    "            tmp = [i for i in res]\n",
    "                # tmp => [7, 7, 7, 7, 7, 7, 7, 4, ..., 7, 7, 7]\n",
    "            tmp[s_i] = min(init_length[s_i], res[s_i] + step)\n",
    "                # tmp[s_i], np.sum(tmp) => 11, 398\n",
    "            if np.sum(tmp) < total_max_length:\n",
    "                res = tmp\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # res (after all above execution) => [11, 11, 7, 10, 11, 11, 11, 4, ..., 7, 7, 7]\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:32:45.965699Z",
     "iopub.status.busy": "2023-03-12T09:32:45.965148Z",
     "iopub.status.idle": "2023-03-12T09:32:46.032012Z",
     "shell.execute_reply": "2023-03-12T09:32:46.029953Z",
     "shell.execute_reply.started": "2023-03-12T09:32:45.965650Z"
    }
   },
   "outputs": [],
   "source": [
    "# dd = MarkdownDataset(train_df, tokenizer, 0, mode='train', parameter=parameter)\n",
    "# collate_fn = Collate(tokenizer)\n",
    "# dl = DataLoader(dd, shuffle=True, batch_size=1,\n",
    "#                                       num_workers=1, pin_memory=True, worker_init_fn=utils.worker_init_fn,\n",
    "#                                       collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:34:00.711492Z",
     "iopub.status.busy": "2023-03-12T09:34:00.710817Z",
     "iopub.status.idle": "2023-03-12T09:34:00.719740Z",
     "shell.execute_reply": "2023-03-12T09:34:00.717834Z",
     "shell.execute_reply.started": "2023-03-12T09:34:00.711439Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = MarkdownModel(utils.get_model_path(args.model_name, res = parameter.model_dir), seq_length=parameter.seq_length, pretrained=True)\n",
    "# for d in dl:\n",
    "#     draw_graph(model, input_data = [d[0],d[1]], expand_nested=True, save_graph=True, device='cpu').visual_graph\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.654104Z",
     "iopub.status.busy": "2023-03-12T09:30:56.653445Z",
     "iopub.status.idle": "2023-03-12T09:30:56.665982Z",
     "shell.execute_reply": "2023-03-12T09:30:56.664855Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.654060Z"
    }
   },
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        input_ids = [x[0] for x in batch]\n",
    "            # x[0] => [     1    291  24233    404   1192    699    275    386   2136   7027.....\n",
    "        attention_mask = [x[1] for x in batch]\n",
    "            # x[1] => [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 .....\n",
    "        target_mask = [x[2] for x in batch]\n",
    "            # x[2] => [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .....\n",
    "        target = [x[3] for x in batch]\n",
    "            # x[3] => [0.0259366  0.         0.         0.         0.         0. ....\n",
    "        # sample_weight = [x[4] for x in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in input_ids])\n",
    "        \n",
    "            # self.tokenizer.pad_token_id => 0\n",
    "        # add padding\n",
    "        input_ids = [list(s) + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in input_ids]\n",
    "        attention_mask = [list(s) + (batch_max - len(s)) * [0] for s in attention_mask]\n",
    "        target_mask = [list(s) + (batch_max - len(s)) * [0] for s in target_mask]\n",
    "        target = [list(s) + (batch_max - len(s)) * [0.] for s in target]\n",
    "        \n",
    "        # sample_weight = [list(s) + (batch_max - len(s)) * [0] for s in target]\n",
    "        \n",
    "        # convert to tensors\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.int32) # torch.long\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.int32) # torch.long\n",
    "        target_mask = torch.tensor(target_mask, dtype=torch.int32) # torch.float32\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        # sample_weight = torch.tensor(sample_weight, dtype=torch.float32)\n",
    "\n",
    "        return input_ids, attention_mask, target_mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.668071Z",
     "iopub.status.busy": "2023-03-12T09:30:56.667471Z",
     "iopub.status.idle": "2023-03-12T09:30:56.686543Z",
     "shell.execute_reply": "2023-03-12T09:30:56.684916Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.668034Z"
    }
   },
   "outputs": [],
   "source": [
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self, name, num_classes=1, seq_length=96, pretrained=True):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        # self.encoder = AutoModel.from_pretrained(name, attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1)\n",
    "        self.config = AutoConfig.from_pretrained(name)\n",
    "        self.config.attention_probs_dropout_prob = 0.\n",
    "        self.config.hidden_dropout_prob = 0.\n",
    "        self.config.max_position_embeddings = 4096 * 2 \n",
    "        # self.config.output_hidden_states = True\n",
    "        if pretrained:\n",
    "            self.encoder = AutoModel.from_pretrained(name, config=self.config, ignore_mismatched_sizes=True)\n",
    "            # self.encoder = AutoModelForMaskedLM.from_pretrained(name, config=self.config)\n",
    "        else:\n",
    "            # self.encoder = AutoModelForMaskedLM.from_config(self.config)\n",
    "            self.encoder = AutoModel.from_config(self.config)\n",
    "\n",
    "        # self.encoder = AutoModel.from_pretrained(name)\n",
    "        # print(self.encoder.__dict__)\n",
    "        # transformer_layers = 2\n",
    "        # self.seq_length = seq_length\n",
    "        # self.transformer_layers = transformer_layers\n",
    "        self.in_dim = self.encoder.config.hidden_size\n",
    "\n",
    "#         self.pe = PositionalEncoding(self.in_dim)\n",
    "#         self.trans = nn.Sequential(\n",
    "#             *[TransformerBlock(emb_s=64, head_cnt=self.in_dim // 64, dp1=0., dp2=0.) for _ in\n",
    "#               range(transformer_layers)])\n",
    "        self.bilstm = nn.LSTM(self.in_dim, self.in_dim, num_layers=1, \n",
    "                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n",
    "                              bidirectional=True)\n",
    "    \n",
    "#         self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "#         hidden = 64\n",
    "#         dropout = 0.\n",
    "#         self.sequence = nn.Sequential(\n",
    "#             # nn.BatchNorm1d(1),\n",
    "#             nn.Linear(1, hidden),  # todo\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.BatchNorm1d(hidden),\n",
    "#             nn.Linear(hidden, hidden),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "        self.last_fc = nn.Linear(self.in_dim*2, num_classes)\n",
    "        # self.fc = nn.LazyLinear(num_classes)\n",
    "        \n",
    "        # function is intended to be used to initialize neural network parameters, -\n",
    "        # - so they all run in torch.no_grad() mode and will not be taken into account by autograd.\n",
    "        # Fills the input Tensor with values drawn from the normal distribution N(mean,std_square).\n",
    "        torch.nn.init.normal_(self.last_fc.weight, std=0.02)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder(x, attention_mask=mask)[\"last_hidden_state\"]\n",
    "        # x = x.reshape(-1, code_count, self.seq_length, self.in_dim).mean(2)\n",
    "        #         x = torch.sum(x * mask.unsqueeze(-1), dim=1) / torch.sum(mask, dim=1).unsqueeze(-1)\n",
    "        #         x = x.reshape(-1, code_count, self.in_dim)\n",
    "        # x = x + self.sequence(dense_features.unsqueeze(-1))\n",
    "\n",
    "#         prev = None\n",
    "#         x = self.pe(x)\n",
    "#         for i in range(self.transformer_layers):\n",
    "#             # x = x * mask.unsqueeze(-1)\n",
    "#             x, prev = self.trans[i](x, prev)\n",
    "        # x = torch.sum(x * mask.unsqueeze(-1), dim=1) / torch.sum(mask, dim=1).unsqueeze(-1)\n",
    "        # x = torch.cat([x, self.sequence(dense_features.unsqueeze(1)).repeat(1,2048,1)], dim=2)\n",
    "        # x = x.mean(1)\n",
    "        x, _ = self.bilstm(x)\n",
    "        out = self.last_fc(x)\n",
    "#         for i, dropout in enumerate(self.dropouts):\n",
    "#             if i == 0:\n",
    "#                 out = self.last_fc(dropout(x))\n",
    "#             else:\n",
    "#                 out += self.last_fc(dropout(x))\n",
    "#         out /= len(self.dropouts)\n",
    "        # out = self.sig(out)\n",
    "        out = out.squeeze(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.689504Z",
     "iopub.status.busy": "2023-03-12T09:30:56.688912Z",
     "iopub.status.idle": "2023-03-12T09:30:56.706515Z",
     "shell.execute_reply": "2023-03-12T09:30:56.704618Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.689453Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, scheduler, optimizer, epoch, n_accumulate=1, is_adversial=False, scaler=None):\n",
    "    batch_time = utils.AverageMeter()\n",
    "    losses = utils.AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    fgm = FGM(model) if is_adversial else None\n",
    "    start = time.time()\n",
    "    k=0    \n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        if parameter.use_cuda:\n",
    "            batch_data = (t.cuda() for t in batch_data)\n",
    "        seq, seq_mask, target_mask, target = batch_data\n",
    "        \n",
    "        with amp.autocast():\n",
    "            output = model(seq, seq_mask)\n",
    "                # output.shape => torch.Size([1, 511])\n",
    "            loss = criterion(output, target, target_mask)\n",
    "            \n",
    "        losses.update(loss.item())\n",
    "        scaler.scale(loss/n_accumulate).backward()\n",
    "        if is_adversial:\n",
    "            # confrontation training\n",
    "            fgm.attack()  # Add anti-perturbation to embedding\n",
    "            output = model(seq, seq_mask)\n",
    "            loss_adv = criterion(output, target, target_mask)\n",
    "            loss_adv.backward()  # Backpropagation, and on the basis of normal grad, accumulate the gradient of confrontation training.\n",
    "            fgm.restore()  # Restore embedding parameters\n",
    "        if args.max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "        if (k + 1) % n_accumulate == 0:\n",
    "            scaler.step(optimizer)     \n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            k=-1\n",
    "            # scheduler.step()\n",
    "        k+=1\n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()\n",
    "        \n",
    "#         if i % parameter.print_freq == 0:\n",
    "#             log.write('Epoch: [{0}][{1}/{2}], Loss {loss:.4f}\\n'.format(epoch, i, len(train_loader), loss=loss.item()))\n",
    "\n",
    "    return losses.avg, batch_time.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.709310Z",
     "iopub.status.busy": "2023-03-12T09:30:56.708667Z",
     "iopub.status.idle": "2023-03-12T09:30:56.723770Z",
     "shell.execute_reply": "2023-03-12T09:30:56.722724Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.709267Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, criterion, valid_loader):#, code_df_valid):\n",
    "    batch_time = utils.AverageMeter()\n",
    "    losses = utils.AverageMeter()\n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    rank_pred = []\n",
    "    masks = []\n",
    "    start = time.time()    \n",
    "    for i, batch_data in enumerate(valid_loader):\n",
    "        if parameter.use_cuda:\n",
    "            batch_data = (t.cuda() for t in batch_data)\n",
    "        seq, seq_mask, target_mask, target = batch_data\n",
    "        outputs = model(seq, seq_mask)\n",
    "        loss = criterion(outputs, target, target_mask)\n",
    "        losses.update(loss.item())\n",
    "        \n",
    "        batch_time.update(time.time() - start)\n",
    "        start = time.time()        \n",
    "        \n",
    "    return losses.avg, batch_time.sum        \n",
    "#         outputs= outputs.detach().cpu().numpy()\n",
    "#             # outputs.shape => (1, 213) \n",
    "#         target_mask = target_mask.detach().cpu().numpy()        \n",
    "#             # target_mask.shape => (1, 213)\n",
    "#         tmp1 = np.zeros((outputs.shape[0], 4096))\n",
    "#             # tmp1.shape => (1, 4096)\n",
    "#         tmp1[:, :outputs.shape[1]] = outputs\n",
    "#         tmp2 = np.zeros((outputs.shape[0], 4096))\n",
    "#             # tmp2.shape => (1, 4096)\n",
    "#         tmp2[:, :outputs.shape[1]] = target_mask\n",
    "#         rank_pred.append(tmp1)\n",
    "#         masks.append(tmp2)\n",
    "#         # print(y_pred)\n",
    "#     rank_pred = np.concatenate(rank_pred)\n",
    "#     masks = np.concatenate(masks)\n",
    "#     # print(y_pred.shape)\n",
    "#     # y_pred = y_pred.reshape((-1, parameter.seq_length))\n",
    "#     score = utils.get_score(valid_loader.dataset.meta_data.copy(), masks, rank_pred, code_df_valid, parameter.data_dir)\n",
    "#         # valid_loader.dataset.meta_data => \n",
    "#         #                   id  id2  fold_flag  cell_count  markdown_count  code_count                                             cell_id     ...\n",
    "#         # 0     0001bdd4021779    1          0          13               2          11   \n",
    "#         # 1     0002115f48f982    1          0           9               1           8    [3fdc37be, 073782ca, 8ea7263c, 80543cd8, 38310...    ...\n",
    "        \n",
    "#     return rank_pred, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.726307Z",
     "iopub.status.busy": "2023-03-12T09:30:56.725034Z",
     "iopub.status.idle": "2023-03-12T09:30:56.742919Z",
     "shell.execute_reply": "2023-03-12T09:30:56.741875Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.726262Z"
    }
   },
   "outputs": [],
   "source": [
    "log_out_dir = os.path.join(parameter.result_dir, 'logs')\n",
    "os.makedirs(log_out_dir, exist_ok=True)\n",
    "log_dir = os.path.join(log_out_dir, '{}.txt'.format(args.model_name))\n",
    "if os.path.exists(log_dir):\n",
    "    os.remove(log_dir)\n",
    "log = utils.Logger()\n",
    "log.open(log_dir, mode='a')\n",
    "\n",
    "model_dir = os.path.join(parameter.result_dir, '')\n",
    "model_dir_seg = os.path.join(parameter.result_dir_1, '')\n",
    "# os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.745630Z",
     "iopub.status.busy": "2023-03-12T09:30:56.744601Z",
     "iopub.status.idle": "2023-03-12T09:30:56.777668Z",
     "shell.execute_reply": "2023-03-12T09:30:56.776246Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.745551Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    log.write('>> parameter:\\n{}\\nargs:{}\\n'.format(parameter, args), is_terminal=0)\n",
    "    # set random seeds\n",
    "    seed_everything(parameter.random_seed)\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    log.write('>> data_processing...\\n')\n",
    "    \n",
    "    # debug = not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "    #     if debug:\n",
    "    #         return 1\n",
    "    # load data\n",
    "    \n",
    "    # train_df, code_df_valid = preprocess.get_data()\n",
    "    train_df = pd.read_parquet(parameter.data_dir+'train_df')\n",
    "    code_df_valid = pd.read_parquet(parameter.data_dir+'code_df_valid')\n",
    "    oof_prediction = np.zeros((len(train_df)))\n",
    "    \n",
    "    eval_loss = []\n",
    "    log.write('---------------------------------------------------------------------------------------------------\\n')\n",
    "    for fold in range(parameter.k_folds):\n",
    "        if len(args.folds) > 0 and str(fold) not in args.folds.split(','):\n",
    "            continue\n",
    "        model = MarkdownModel(utils.get_model_path(args.model_name, res = parameter.model_dir), seq_length=parameter.seq_length, pretrained=True)\n",
    "        model.zero_grad()\n",
    "        if parameter.use_cuda:\n",
    "            model = model.cuda()\n",
    "            # p =>\n",
    "            # Parameter containing:\n",
    "            # tensor([[-0.0504, -0.0459, -0.0496,  ..., -0.0528, -0.0523, -0.0369],\n",
    "            #         [-0.0017, -0.0081,  0.0130,  ..., -0.0137, -0.0064, -0.0035],\n",
    "            #         ...,\n",
    "            #         [-0.0448, -0.0498, -0.0464,  ..., -0.0550, -0.0567, -0.0458],\n",
    "            #         [-0.0400, -0.0428, -0.0420,  ..., -0.0588, -0.0488, -0.0336]],\n",
    "            #        requires_grad=True)\n",
    "            # p.numel() => 131174400\n",
    "        total_params = sum(p.numel() for p in model.parameters())        \n",
    "            # total_params => 450807809\n",
    "\n",
    "        log.write('model total parameters:{}\\n'.format(total_params), is_terminal=0)\n",
    "        total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            # total_trainable_params => 450807809\n",
    "        log.write('model total training parameters:{}\\n'.format(total_trainable_params), is_terminal=0)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(utils.get_model_path(args.model_name, res = parameter.model_dir))\n",
    "\n",
    "        train_model_filename = os.path.join(model_dir_seg, args.model_name + '_fold{}_fnl.pth.tar'.format(fold))\n",
    "            # train_model_filename => models/deberta-v3-large_fold0.pth.tar\n",
    "        if not os.path.exists(train_model_filename):\n",
    "            # model.load_state_dict(torch.load(train_model_filename, map_location='cpu')['state_dict'])\n",
    "            filename = os.path.join(model_dir_seg, args.model_name + '_fold{}.pth.tar'.format(fold))\n",
    "            if os.path.exists(filename):\n",
    "#                 model.encoder.load_state_dict(torch.load(filename)['state_dict'])\n",
    "                model.load_state_dict(torch.load(filename)['state_dict'], strict=False)                \n",
    "                log.write('loaded pre-trained model\\n')\n",
    "            else:\n",
    "                log.write('not loaded pre-trained model\\n')\n",
    "\n",
    "            collate_fn = Collate(tokenizer)\n",
    "            train_dataset = MarkdownDataset(train_df, tokenizer, fold, mode='train', parameter=parameter)\n",
    "            train_loader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size,\n",
    "                                      num_workers=args.workers, pin_memory=True, worker_init_fn=utils.worker_init_fn,\n",
    "                                      collate_fn=collate_fn)\n",
    "            valid_dataset = MarkdownDataset(train_df, tokenizer, fold, mode='valid', parameter=parameter)\n",
    "            valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=args.batch_size,\n",
    "                                      num_workers=args.workers, pin_memory=True, collate_fn=collate_fn)\n",
    "            criterion = utils.MyLoss()  # MyBCELoss()  # MyLoss()\n",
    "\n",
    "            best_score = -1\n",
    "            patience_cnt = 0\n",
    "            is_improved = True\n",
    "            optimizer = AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.999), eps=1e-6, weight_decay=args.weight_decay)\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.base_epoch, eta_min=args.eta_min)#args.lr / 5\n",
    "            # scheduler = get_constant_schedule_with_warmup(optimizer, 100)\n",
    "            # scheduler = get_cosine_schedule_with_warmup(optimizer, 100, num_training_steps= len(train_loader))\n",
    "            # log.write('not loaded trained model\\n')\n",
    "            scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "            for epoch in range(args.base_epoch):\n",
    "                # adjust learning rate for each epoch\n",
    "                # adjust_learning_rate(optimizer, epoch, parameter)\n",
    "                scheduler.step(epoch=epoch)\n",
    "                log.write('Fold: [{0}] Epoch: [{1}], lr:[{2}]\\n'.format(fold, epoch, optimizer.param_groups[0]['lr']))\n",
    "                is_adversial = False\n",
    "                train_loss, train_batch_time = train(model, train_loader, criterion, scheduler, optimizer, epoch,\n",
    "                                                     args.n_accumulate, is_adversial, scaler)\n",
    "                log.write('Epoch avg loss: {0}, Epoch cost time:{1} min\\n'.format(train_loss, train_batch_time / 60))\n",
    "                with torch.no_grad():                    \n",
    "                    utils.save_model(model, model_dir, '{}_fold{}'.format(args.model_name, fold))                    \n",
    "                    valid_loss, valid_batch_time = validate(model, criterion, valid_loader)#, code_df_valid)\n",
    "                    log.write('Epoch avg loss: {0}, Epoch cost time:{1} min\\n'.format(valid_loss, valid_batch_time / 60))                    \n",
    "#                     rank_pred, score = validate(model, valid_loader, code_df_valid)\n",
    "#                     log.write('Epoch score: {0}\\n'.format(score))\n",
    "#                     if score > -1:  # best_score:\n",
    "#                         best_score, best_epoch = score, epoch\n",
    "#                         # oof_prediction[np.where(train_df['fold_flag'] == fold)] = rank_pred.copy()\n",
    "#                         utils.save_model(model, model_dir, '{}_fold{}'.format(args.model_name, fold))\n",
    "#                         log.write('********Best Epoch: [{0}], Best Score:{1}********\\n'.format(best_epoch, best_score))\n",
    "#                     else:\n",
    "#                         is_improved = False\n",
    "#                         patience_cnt += 1\n",
    "        else:\n",
    "            valid_dataset = MarkdownDataset(train_df, tokenizer, fold, mode='valid', parameter=parameter)\n",
    "            valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=args.batch_size * 4,\n",
    "                                      num_workers=args.workers, pin_memory=True)\n",
    "            model.load_state_dict(torch.load(train_model_filename, map_location='cpu')['state_dict'], strict=False)\n",
    "            model.cuda()\n",
    "            log.write('loaded trained model\\n')\n",
    "            with torch.no_grad():\n",
    "                rank_pred, score = validate(model, valid_loader, code_df_valid)\n",
    "            log.write('Epoch score: {0}\\n'.format(score))\n",
    "            best_score = score\n",
    "            # oof_prediction[np.where(train_df['fold_flag'] == fold)] = rank_pred.copy()\n",
    "        # utils.save_model(model, model_dir,'{}_fold{}'.format(args.model_name, fold))\n",
    "#         eval_loss.append(best_score)\n",
    "        del model\n",
    "        _ = gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "#     log.write('CV mean:{} std:{}.'.format(np.mean(eval_loss), np.std(eval_loss)))\n",
    "#     log.write('detail:{}'.format(np.round(eval_loss, 4)))\n",
    "#     np.save(os.path.join(parameter.result_dir, args.model_name + '_oof.npy'), oof_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-12T09:30:56.779485Z",
     "iopub.status.busy": "2023-03-12T09:30:56.779121Z",
     "iopub.status.idle": "2023-03-12T09:31:44.143001Z",
     "shell.execute_reply": "2023-03-12T09:31:44.141149Z",
     "shell.execute_reply.started": "2023-03-12T09:30:56.779451Z"
    }
   },
   "outputs": [],
   "source": [
    "# model, valid_loader, code_df_valid = main()\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
