{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:58:51.191970Z",
     "iopub.status.busy": "2023-03-16T04:58:51.191359Z",
     "iopub.status.idle": "2023-03-16T04:58:51.196392Z",
     "shell.execute_reply": "2023-03-16T04:58:51.195401Z",
     "shell.execute_reply.started": "2023-03-16T04:58:51.191910Z"
    }
   },
   "outputs": [],
   "source": [
    "# # !pip3 install -q -U huggingface-hub==0.13.0\n",
    "# !pip3 install -q -U graphviz==0.20.1\n",
    "# !pip3 install -q torchview==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:58:51.213034Z",
     "iopub.status.busy": "2023-03-16T04:58:51.212193Z",
     "iopub.status.idle": "2023-03-16T04:58:51.217745Z",
     "shell.execute_reply": "2023-03-16T04:58:51.216665Z",
     "shell.execute_reply.started": "2023-03-16T04:58:51.212997Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['../input/nar-ai4code-utils'] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:58:51.224655Z",
     "iopub.status.busy": "2023-03-16T04:58:51.224388Z",
     "iopub.status.idle": "2023-03-16T04:58:51.228692Z",
     "shell.execute_reply": "2023-03-16T04:58:51.227556Z",
     "shell.execute_reply.started": "2023-03-16T04:58:51.224630Z"
    }
   },
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# from torchview import draw_graph\n",
    "# graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:58:51.244086Z",
     "iopub.status.busy": "2023-03-16T04:58:51.242195Z",
     "iopub.status.idle": "2023-03-16T04:58:53.768734Z",
     "shell.execute_reply": "2023-03-16T04:58:53.767654Z",
     "shell.execute_reply.started": "2023-03-16T04:58:51.244057Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-16T04:58:53.771063Z",
     "iopub.status.busy": "2023-03-16T04:58:53.770585Z",
     "iopub.status.idle": "2023-03-16T04:59:01.235314Z",
     "shell.execute_reply": "2023-03-16T04:59:01.234201Z",
     "shell.execute_reply.started": "2023-03-16T04:58:53.771033Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "# from utils import create_label\n",
    "import numpy as np\n",
    "\n",
    "# coding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time, gc, argparse, shutil\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from transformers import get_constant_schedule_with_warmup, AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel, AutoModelForMaskedLM\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.237624Z",
     "iopub.status.busy": "2023-03-16T04:59:01.236905Z",
     "iopub.status.idle": "2023-03-16T04:59:01.251588Z",
     "shell.execute_reply": "2023-03-16T04:59:01.250612Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.237592Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch')\n",
    "parser.add_argument('--out_dir', default='', type=str, help='destination where trained network should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for training')\n",
    "parser.add_argument('--model_name', default='deberta-v3-base', type=str)\n",
    "parser.add_argument('--base_epoch', default=1, type=int, help='number of total epochs to run')#30\n",
    "parser.add_argument('--batch_size', default=2, type=int, help='train mini-batch size')# 8\n",
    "parser.add_argument('--workers', default=2, type=int, help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--lr', default=17e-6, type=float, help='learning rate') # 5e-6\n",
    "parser.add_argument('--eta_min', default=11e-6, type=float, help='learning rate eta_min') # 5e-6\n",
    "\n",
    "# gradient accumulation steps as the batch_size=1 then set this to 5.\n",
    "parser.add_argument('--n_accumulate', default=5, type=int) \n",
    "\n",
    "parser.add_argument('--max_grad_norm', default=-1, type=float) # 1.0\n",
    "parser.add_argument('--weight_decay', default=0.0, type=float)\n",
    "parser.add_argument('--folds', default='', type=str)\n",
    "parser.add_argument('--pre_epoch', default=15, type=int)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.255165Z",
     "iopub.status.busy": "2023-03-16T04:59:01.254750Z",
     "iopub.status.idle": "2023-03-16T04:59:01.265439Z",
     "shell.execute_reply": "2023-03-16T04:59:01.264477Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.255128Z"
    }
   },
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self):\n",
    "        # data\n",
    "        self.result_dir = './models/' # ./user_data/\n",
    "        self.result_dir_1 = '../input/nar-ai4code-models/'        \n",
    "        self.data_dir = './'\n",
    "        self.model_dir = '../input/nar-transformers/'\n",
    "        self.k_folds = 5\n",
    "        self.random_seed = 27\n",
    "        self.seq_length = 512 # 512; sum of \"cell length\" for a particular id.\n",
    "        self.seq_max_length = 4096  \n",
    "        self.cell_max_length = 128 # max length of a cell.\n",
    "        self.cell_count = 128        \n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        # model\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.gpu = 0\n",
    "        self.print_freq = 500\n",
    "        self.weight_decay = 0\n",
    "        self.optim = 'Adam'\n",
    "\n",
    "    def get(self, name):\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def set(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(['%s:%s' % item for item in self.__dict__.items()])\n",
    "    \n",
    "parameter = Parameter()\n",
    "parameter.set(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.268190Z",
     "iopub.status.busy": "2023-03-16T04:59:01.267516Z",
     "iopub.status.idle": "2023-03-16T04:59:01.278062Z",
     "shell.execute_reply": "2023-03-16T04:59:01.277144Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.268155Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
    "#     if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    #         torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False        \n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.282038Z",
     "iopub.status.busy": "2023-03-16T04:59:01.281555Z",
     "iopub.status.idle": "2023-03-16T04:59:01.308431Z",
     "shell.execute_reply": "2023-03-16T04:59:01.307467Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.282010Z"
    }
   },
   "outputs": [],
   "source": [
    "class MarkdownDataset(Dataset):\n",
    "\n",
    "    def __init__(self, meta_data: pd.DataFrame, tokenizer, fold: int = -1, mode='train', parameter=None):\n",
    "        self.meta_data = meta_data.copy()\n",
    "        if mode == 'train':\n",
    "            pass\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] != fold].copy()\n",
    "#             self.meta_data = self.meta_data.iloc[:60000]\n",
    "            self.meta_data.reset_index(drop=True, inplace=True)\n",
    "        elif mode == 'valid':\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] == fold].copy()\n",
    "#             self.meta_data = self.meta_data[self.meta_data['id'].isin(self.meta_data['id'].values[:1000])]\n",
    "            self.meta_data.reset_index(drop=True, inplace=True)\n",
    "        elif mode == 'test':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(mode)\n",
    "\n",
    "        if tokenizer.sep_token != '[SEP]':\n",
    "            self.meta_data['source'] = self.meta_data['source'].apply(\n",
    "                lambda x: [\n",
    "                    y.replace(tokenizer.sep_token, '').replace(tokenizer.cls_token, '').replace(tokenizer.pad_token, '')\n",
    "                    for y in x])\n",
    "        self.parameter = parameter        \n",
    "        self.seq_max_length = self.parameter.seq_max_length\n",
    "        self.source = self.meta_data['source'].values\n",
    "        self.cell_type = self.meta_data['cell_type'].values\n",
    "        self.batch_max_length = self.meta_data['batch_max_length'].values        \n",
    "        # self.cell_id = self.meta_data['cell_id'].values\n",
    "        # self.rank = self.meta_data['rank'].values\n",
    "        # self.dense_features = self.meta_data[['cell_count','markdown_count', 'code_count']].values\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = self.source[index]\n",
    "            # (no data cleaning) source =>\n",
    "            # array([' this python 3 environment .... the current session', ..., ' на обучении на один признак больше чем на тесте '], dtype=object)\n",
    "            \n",
    "            # [len(x) for x in source] => \n",
    "            # [845, 491, 47, 40, 555, 154, 54, 15, 141, 51, 74, 35, ..., 62, 34, 39, 49]            \n",
    "        \n",
    "            # max([len(x) for x in source]), min([len(x) for x in source]) => 3722, 10\n",
    "        cell_type = self.cell_type[index]\n",
    "            # cell_type =>\n",
    "            # array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ....,  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])        \n",
    "        \n",
    "#         rank = self.rank[index]\n",
    "#             # rank => array([0.01724138, 0.05172414, 0.0862069 , 0.12068966, 0.15517241, ..., 0.93103448, 0.62068966, 0.4137931 ]) \n",
    "            \n",
    "        batch_max_len = min(self.batch_max_length[index], self.seq_max_length)            \n",
    "    \n",
    "        # dense_features = 1#self.dense_features[index]\n",
    "        # if self.mode == 'train':\n",
    "        #     range_tmp1 = [ i for i in range(len(cell_type)) if cell_type[i]==0]\n",
    "        #     range_tmp2 = [ i for i in range(len(cell_type)) if cell_type[i]==1]\n",
    "        #     np.random.shuffle(range_tmp2)\n",
    "        #     source = [source[i] for i in range_tmp1 + range_tmp2]\n",
    "        #     rank = [rank[i] for i in range_tmp1 + range_tmp2]\n",
    "        #     rank2 = [rank2[i] for i in range_tmp1 + range_tmp2]\n",
    "\n",
    "        cell_inputs = self.tokenizer(#.batch_encode_plus\n",
    "            source,#.tolist(),\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.parameter.cell_max_length,\n",
    "            # padding=\"max_length\",\n",
    "            return_attention_mask=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "            # cell_inputs =>\n",
    "            # {'input_ids': [[291, 24233, 404, ..., 2, 763, 5514, 4219], ..., [15003, 42120, 69819, ..., 15003, 65639, 109249]],\n",
    "            # 'token_type_ids': [[0, 0, 0, ..., 0, 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0]]}\n",
    "\n",
    "            # [len(x) for x in cell_inputs['input_ids']] => [128, 105, 7, 10, 127, 28, 12, 4, 26, 12, 23, 9, ... , 18]        \n",
    "            # max([len(x) for x in cell_inputs['input_ids']]), min([len(x) for x in cell_inputs['input_ids']]) => 128, 4        \n",
    "        \n",
    "        seq, seq_mask, target_mask = self.max_length_rule_base(cell_inputs['input_ids'], cell_type, batch_max_len)# rank)\n",
    "        \n",
    "        # if self.mode == 'train':\n",
    "        #     attention_mask, target = self.random_mask(attention_mask, target)\n",
    "        return seq, seq_mask, target_mask\n",
    "        # return encoded['input_ids'][0], encoded['attention_mask'][0], np.array(target, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)\n",
    "\n",
    "    def max_length_rule_base(self, cell_inputs, cell_type, batch_max_len):# rank):\n",
    "        init_length = [len(x) for x in cell_inputs]\n",
    "            # init_length => [128, 105, 7, ..., 13, 16, 18]\n",
    "            # batch_max_len => 4096 \n",
    "            # len(init_length) => 128\n",
    "\n",
    "        total_max_length = batch_max_len - len(init_length)\n",
    "            # total_max_length => 3968           \n",
    "        min_length = total_max_length // len(init_length)\n",
    "            # min_length => 31\n",
    "                         \n",
    "        cell_length = self.search_length(init_length, min_length, total_max_length, len(init_length))\n",
    "            # cell_length => [51, 51, 51, 51, 41, 51, 51, 51, 51, 51, 42, 51, 11, 51, 51, 51 ....\n",
    "            # len(cell_length) => 128\n",
    "\n",
    "        seq = []\n",
    "        for i in range(len(cell_length)):\n",
    "            if cell_type[i] == 0:\n",
    "                seq.append(self.tokenizer.cls_token_id)\n",
    "                # self.tokenizer.cls_token_id => 1 \n",
    "            else:\n",
    "                seq.append(self.tokenizer.sep_token_id)\n",
    "                # self.tokenizer.sep_token_id => 2\n",
    "            if cell_length[i] > 0:\n",
    "                seq.extend(cell_inputs[i][:cell_length[i]])\n",
    "\n",
    "            # cell_inputs[i] => [291, 24233, 404, ..., 2, 763, 5514, 4219]\n",
    "            # seq => [1, 291, 24233, 404, 1192, 699, 275, 386, 2136, 7027, 7296, 2627] \n",
    "            \n",
    "        # seq => [1, 291, 24233, 404, 1192, 699, 275, 386, 2136, 7027, 7296, 2627, 1, 6306, ..., 2, 15003, ..., 88580]\n",
    "        \n",
    "        # seq_mask or attention_mask tells model which position should it attend in seq. 1 -> attend and 0 -> not attend.\n",
    "        # tokenizer.pad_token_id => 0\n",
    "        if len(seq) < batch_max_len:\n",
    "            seq_mask = [1] * len(seq) + [0] * (batch_max_len - len(seq))\n",
    "            seq = seq + [self.tokenizer.pad_token_id] * (batch_max_len - len(seq))\n",
    "        else:\n",
    "            seq_mask = [1] * batch_max_len\n",
    "            seq = seq[:batch_max_len]\n",
    "            \n",
    "        # seq_mask is attention_mask.\n",
    "        seq, seq_mask = np.array(seq, dtype=np.int32), np.array(seq_mask, dtype=np.int32)\n",
    "        target_mask = np.where((seq == self.tokenizer.cls_token_id) | (seq == self.tokenizer.sep_token_id), 1, 0) \n",
    "            # target_mask => [1 0 0 0 0 0 0 0 0 0 0 0 1 ..... 0 0 0 0]            \n",
    "            \n",
    "#         target = np.zeros(len(seq), dtype=np.float32)\n",
    "        \n",
    "#         # return indices where self.tokenizer.cls_token_id or self.tokenizer.sep_token_id is present\n",
    "#         tmp = np.where((seq == self.tokenizer.cls_token_id) | (seq == self.tokenizer.sep_token_id))\n",
    "#             # tmp => (array([  0,  12,  24,  32,  43,  55,  67,  79,  ..., 479, 487, 495, 503]),)        \n",
    "#         target[tmp] = rank\n",
    "#             # target => [0.01724138 0.         0.         0.    ...      0.        ]\n",
    "            \n",
    "#         sample_weight = np.zeros(len(seq), dtype=np.float32)\n",
    "#         sample_weight = np.where(seq == self.tokenizer.cls_token_id, 0.33, sample_weight)\n",
    "#         sample_weight = np.where(seq == self.tokenizer.sep_token_id, 1.0, sample_weight)\n",
    "#         dense_features = np.zeros(batch_max_len, dtype=np.float32)\n",
    "#         dense_features[tmp] = rank2\n",
    "\n",
    "        return seq, seq_mask, target_mask#, target\n",
    "\n",
    "    # Static method objects provide a way of defeating the transformation of function objects to method objects.\n",
    "    # A static method does not receive an implicit first argument. we can even call it from an instance. \n",
    "    # A class method receives the class as implicit first argument, just like an instance method receives the instance.\n",
    "    # The insertion of @staticmethod before a method definition, then, stops an instance from sending itself as an argument.\n",
    "    @staticmethod\n",
    "    def search_length(init_length, min_length, total_max_length, cell_count, step=5, max_search_count=50):# step=4\n",
    "            # init_length => [128, 128, 97, 68, 41, 60, 60, 119, 66, 127, ...\n",
    "            # np.sum(init_length) => 6377\n",
    "            # min_length, total_max_length, cell_count => 31, 3968, 128\n",
    "            # len(init_length) is cell_count.\n",
    "            \n",
    "        if np.sum(init_length) <= total_max_length:\n",
    "            return init_length\n",
    "\n",
    "        res = [min(init_length[i], min_length) for i in range(cell_count)]      \n",
    "            # res => [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, ...\n",
    "        for s_i in range(max_search_count):\n",
    "            tmp = [min(init_length[i], res[i] + step) for i in range(cell_count)]# step=4\n",
    "                # tmp => [36, 36, 36, 36, 36, 36, 36, 36, ...\n",
    "                # np.sum(tmp) => 3234                                 \n",
    "            if np.sum(tmp) < total_max_length:\n",
    "                res = tmp\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        for s_i in range(cell_count):\n",
    "            tmp = [i for i in res]\n",
    "                # tmp => [46, 46, 46, 46, 41, 46, 46, 46, ...                 \n",
    "            tmp[s_i] = min(init_length[s_i], res[s_i] + step)\n",
    "                # tmp[s_i], np.sum(tmp) => 51, 3906                \n",
    "            if np.sum(tmp) < total_max_length:\n",
    "                res = tmp\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # res (after all above execution) => [51, 51, 51, 51, 41, 51, 51, 51, 51 ...\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.311239Z",
     "iopub.status.busy": "2023-03-16T04:59:01.310150Z",
     "iopub.status.idle": "2023-03-16T04:59:01.323121Z",
     "shell.execute_reply": "2023-03-16T04:59:01.322106Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.311203Z"
    }
   },
   "outputs": [],
   "source": [
    "class MarkdownModel(nn.Module):\n",
    "    def __init__(self, name, num_classes=1, seq_length=96, pretrained=True):\n",
    "        super(MarkdownModel, self).__init__()\n",
    "        # self.encoder = AutoModel.from_pretrained(name, attention_probs_dropout_prob=0.1, hidden_dropout_prob=0.1)\n",
    "        self.config = AutoConfig.from_pretrained(name)\n",
    "        self.config.attention_probs_dropout_prob = 0.\n",
    "        self.config.hidden_dropout_prob = 0.\n",
    "        self.config.max_position_embeddings = 4096 * 2 \n",
    "        # self.config.output_hidden_states = True\n",
    "        if pretrained:\n",
    "            self.encoder = AutoModel.from_pretrained(name, config=self.config, ignore_mismatched_sizes=True)\n",
    "            # self.encoder = AutoModelForMaskedLM.from_pretrained(name, config=self.config)\n",
    "        else:\n",
    "            # self.encoder = AutoModelForMaskedLM.from_config(self.config)\n",
    "            self.encoder = AutoModel.from_config(self.config)\n",
    "\n",
    "        # self.encoder = AutoModel.from_pretrained(name)\n",
    "        # print(self.encoder.__dict__)\n",
    "        # transformer_layers = 2\n",
    "        # self.seq_length = seq_length\n",
    "        # self.transformer_layers = transformer_layers\n",
    "        self.in_dim = self.encoder.config.hidden_size\n",
    "\n",
    "#         self.pe = PositionalEncoding(self.in_dim)\n",
    "#         self.trans = nn.Sequential(\n",
    "#             *[TransformerBlock(emb_s=64, head_cnt=self.in_dim // 64, dp1=0., dp2=0.) for _ in\n",
    "#               range(transformer_layers)])\n",
    "        self.bilstm = nn.LSTM(self.in_dim, self.in_dim, num_layers=1, \n",
    "                              dropout=self.config.hidden_dropout_prob, batch_first=True,\n",
    "                              bidirectional=True)\n",
    "    \n",
    "#         self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "#         hidden = 64\n",
    "#         dropout = 0.\n",
    "#         self.sequence = nn.Sequential(\n",
    "#             # nn.BatchNorm1d(1),\n",
    "#             nn.Linear(1, hidden),  # todo\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.ReLU(),\n",
    "#             # nn.BatchNorm1d(hidden),\n",
    "#             nn.Linear(hidden, hidden),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "\n",
    "        self.last_fc = nn.Linear(self.in_dim*2, num_classes)\n",
    "        # self.fc = nn.LazyLinear(num_classes)\n",
    "        \n",
    "        # function is intended to be used to initialize neural network parameters, -\n",
    "        # - so they all run in torch.no_grad() mode and will not be taken into account by autograd.\n",
    "        # Fills the input Tensor with values drawn from the normal distribution N(mean,std_square).\n",
    "        torch.nn.init.normal_(self.last_fc.weight, std=0.02)\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.encoder(x, attention_mask=mask)[\"last_hidden_state\"]\n",
    "        # x = x.reshape(-1, code_count, self.seq_length, self.in_dim).mean(2)\n",
    "        #         x = torch.sum(x * mask.unsqueeze(-1), dim=1) / torch.sum(mask, dim=1).unsqueeze(-1)\n",
    "        #         x = x.reshape(-1, code_count, self.in_dim)\n",
    "        # x = x + self.sequence(dense_features.unsqueeze(-1))\n",
    "\n",
    "#         prev = None\n",
    "#         x = self.pe(x)\n",
    "#         for i in range(self.transformer_layers):\n",
    "#             # x = x * mask.unsqueeze(-1)\n",
    "#             x, prev = self.trans[i](x, prev)\n",
    "        # x = torch.sum(x * mask.unsqueeze(-1), dim=1) / torch.sum(mask, dim=1).unsqueeze(-1)\n",
    "        # x = torch.cat([x, self.sequence(dense_features.unsqueeze(1)).repeat(1,2048,1)], dim=2)\n",
    "        # x = x.mean(1)\n",
    "        x, _ = self.bilstm(x)\n",
    "        out = self.last_fc(x)\n",
    "#         for i, dropout in enumerate(self.dropouts):\n",
    "#             if i == 0:\n",
    "#                 out = self.last_fc(dropout(x))\n",
    "#             else:\n",
    "#                 out += self.last_fc(dropout(x))\n",
    "#         out /= len(self.dropouts)\n",
    "        # out = self.sig(out)\n",
    "        out = out.squeeze(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.326285Z",
     "iopub.status.busy": "2023-03-16T04:59:01.325541Z",
     "iopub.status.idle": "2023-03-16T04:59:01.337855Z",
     "shell.execute_reply": "2023-03-16T04:59:01.336913Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.326247Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, valid_loader, max_length=4096):\n",
    "    # batch_time = AverageMeter()\n",
    "    # losses = AverageMeter()\n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    rank_pred = []\n",
    "    masks = []\n",
    "    for i, batch_data in enumerate(valid_loader):\n",
    "        if parameter.use_cuda:\n",
    "            batch_data = (t.cuda() for t in batch_data)\n",
    "        seq, seq_mask, target_mask = batch_data\n",
    "            # seq.shape, seq_mask.shape => torch.Size([1, 213]), torch.Size([1, 213])\n",
    "        outputs = model(seq, seq_mask)\n",
    "        outputs= outputs.detach().cpu().numpy()\n",
    "            # outputs.shape => (1, 213) \n",
    "        target_mask = target_mask.detach().cpu().numpy()#.reshape((outputs.shape[0], -1))       \n",
    "            # target_mask.shape => (1, 213)\n",
    "        \n",
    "        # taken 4096; different sequences have different shapes so taken maximum length -\n",
    "        # - otherwise np.concatenate does not concat different shapes.\n",
    "        tmp1 = np.zeros((outputs.shape[0], max_length)) \n",
    "            # tmp1.shape => (1, 4096)\n",
    "        tmp1[:, :outputs.shape[1]] = outputs\n",
    "        tmp2 = np.zeros((outputs.shape[0], max_length))\n",
    "            # tmp2.shape => (1, 4096)\n",
    "        tmp2[:, :outputs.shape[1]] = target_mask\n",
    "        \n",
    "        rank_pred.append(tmp1)\n",
    "        masks.append(tmp2)\n",
    "\n",
    "    \n",
    "    rank_pred = np.concatenate(rank_pred)\n",
    "        # rank_pred.shape (BS, 4096)\n",
    "    masks = np.concatenate(masks)\n",
    "        # masks.shape (BS, 4096)\n",
    "\n",
    "    return rank_pred, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.340986Z",
     "iopub.status.busy": "2023-03-16T04:59:01.339824Z",
     "iopub.status.idle": "2023-03-16T04:59:01.351117Z",
     "shell.execute_reply": "2023-03-16T04:59:01.350140Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.340949Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds(my_df, my_loader, my_model, model_path, max_length=4096):\n",
    "    if my_df.shape[0] > 0:\n",
    "        my_model.load_state_dict(torch.load(model_path)['state_dict'])\n",
    "    if parameter.use_cuda:\n",
    "        my_model = my_model.cuda()\n",
    "    with torch.no_grad():\n",
    "        y_pred, mask = validate(my_model, my_loader, max_length)\n",
    "    return y_pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.355714Z",
     "iopub.status.busy": "2023-03-16T04:59:01.355338Z",
     "iopub.status.idle": "2023-03-16T04:59:01.366394Z",
     "shell.execute_reply": "2023-03-16T04:59:01.365261Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.355683Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/anyai28/fast-inference-by-padding-optimization\n",
    "def get_sorted_test_df(df, extract_col, tokenizer, batch_size, cell_max_length=128):\n",
    "    input_lengths = []\n",
    "        # batch_size => 2\n",
    "        # len(df[extract_col].values) => len of rows in train/test dataframe => 10\n",
    "    for text in df[extract_col].values:        \n",
    "            # text[0] => import numpy as np linear algebra import pandas as pd pd set option display ...        \n",
    "        tmp = tokenizer(#.batch_encode_plus(\n",
    "            text,#.tolist(),\n",
    "            add_special_tokens=False,\n",
    "            max_length=cell_max_length,\n",
    "            return_attention_mask=False,\n",
    "            truncation=True,\n",
    "        )\n",
    "            # tmp['input_ids'][0] => [6306, 67927, 283, 845, 407, 6306, 36221, 11751, 283, 76767]\n",
    "        \n",
    "        init_length = [len(x) for x in tmp['input_ids']]\n",
    "            # init_length; lengths of cells  in particular input_id\n",
    "            # init_length => [128, 3, 19, 118, 22, 45, 53, 36, 4, 36, 4, 14, 5 ...\n",
    "        \n",
    "        total_length = np.sum(init_length) + len(init_length)\n",
    "            # total_length; sum of words plus \"number of groups in which these words are divided\" (for a particular id).\n",
    "            # np.sum(init_length), len(init_length) => 2908, 93\n",
    "\n",
    "        input_lengths.append(total_length)\n",
    "\n",
    "    df['input_lengths'] = input_lengths\n",
    "        # input_lengths => [3001, 213, 6505, 6223, 5624, 206, 1238, 482, 1052, 1617]\n",
    "    length_sorted_idx = np.argsort([-l for l in input_lengths])\n",
    "        # [-l for l in input_lengths] => [-3001, -213, -6505, -6223, -5624, -206, -1238, -482, -1052, -1617]\n",
    "        # length_sorted_idx => [2 3 4 0 9 6 8 7 1 5]\n",
    "\n",
    "    # sort dataframe\n",
    "    df = df.iloc[length_sorted_idx]\n",
    "    \n",
    "    # calc max_len per batch\n",
    "    sorted_input_length = df['input_lengths'].values\n",
    "        # sorted_input_length => [6505 6223 5624 3001 1617 1238 1052  482  213  206]\n",
    "    batch_max_length = np.zeros_like(sorted_input_length)\n",
    "        # zeros_like; Return an array of zeros with the same shape and type as a given array.\n",
    "        # batch_max_length => [0 0 0 0 0 0 0 0 0 0]\n",
    "    total_iter = len(sorted_input_length) // batch_size if len(sorted_input_length) % batch_size == 0 else (len(sorted_input_length) // batch_size) + 1\n",
    "        # total_iter => 5\n",
    "    for i in range(total_iter):\n",
    "        batch_max_length[i * batch_size:(i + 1) * batch_size] = np.max(sorted_input_length[i * batch_size:(i + 1) * batch_size])\n",
    "    \n",
    "    df['batch_max_length'] = batch_max_length\n",
    "        # batch_max_length => [6505 6505 5624 5624 1617 1617 1052 1052  213  213]  \n",
    "        # 6505; maximum number of words in corresponding row id. \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.369184Z",
     "iopub.status.busy": "2023-03-16T04:59:01.368452Z",
     "iopub.status.idle": "2023-03-16T04:59:01.387104Z",
     "shell.execute_reply": "2023-03-16T04:59:01.386298Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.369140Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(df, masks, rank_pred, code_df_valid):\n",
    "    #     df['cell_id2'] = [[y[i] for i in range(len(x)) if x[i] == 1] for x, y in\n",
    "    #                           zip(df['cell_type'].values, df['cell_id'].values)]\n",
    "    df['cell_id2'] = df['cell_id']\n",
    "    df = df[['id', 'cell_id2']].explode('cell_id2')\n",
    "        # df.head(2) =>\n",
    "        #                    id  cell_id2\n",
    "        # 0      0001bdd4021779  3fdc37be\n",
    "        # 0      0001bdd4021779  073782ca\n",
    "\n",
    "        # pd.isnull(df['cell_id2']) =>\n",
    "        # 0       False\n",
    "        # 0       False\n",
    "        #         ...\n",
    "        # Name: cell_id2, Length: 52282, dtype: bool\n",
    "    df = df[~pd.isnull(df['cell_id2'])]\n",
    "        # rank_pred.flatten()[0:10]\n",
    "        # [-0.01354445  0.06333275  0.09860276  0.12805334  0.04778271  0.03880716 ...\n",
    "        \n",
    "        # masks.flatten()[0:10] =>\n",
    "        # [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "    preds = rank_pred.flatten()[np.where(masks.flatten() == 1)]        \n",
    "        # preds.flatten()[0:10]\n",
    "        # [-0.01354445 -0.01515285 -0.01671559  0.00495259 -0.02522139 -0.02115827\n",
    "    df['rank2'] = preds\n",
    "        # df.groupby(by=['id', 'cell_id2'], as_index=False)['rank2'].head(2) => \n",
    "        # 0      -0.013544\n",
    "        # 0      -0.015153\n",
    "        #         ...\n",
    "        # Name: rank2, Length: 47718, dtype: float64\n",
    "\n",
    "        # ( showing all rows of id 0001bdd4021779 ), df.head(10) => \n",
    "        #                id  cell_id2     rank2\n",
    "        # 0  0001bdd4021779  3fdc37be -0.013544\n",
    "        # 0  0001bdd4021779  073782ca -0.015153\n",
    "        # 0  0001bdd4021779  8ea7263c -0.016716\n",
    "        # 0  0001bdd4021779  80543cd8  0.004953\n",
    "        # 0  0001bdd4021779  38310c80 -0.025221\n",
    "        # 0  0001bdd4021779  073e27e5 -0.021158\n",
    "        # 0  0001bdd4021779  015d52a4  0.042571\n",
    "        # 0  0001bdd4021779  ad7679ef  0.043002\n",
    "        # 0  0001bdd4021779  07c52510 -0.010163\n",
    "        # 0  0001bdd4021779  0a1a7a39  0.006047\n",
    "        # 0  0001bdd4021779  0bcd3fef -0.015030\n",
    "        # 0  0001bdd4021779  7fde4f04  0.083458\n",
    "        # 0  0001bdd4021779  58bf360b  0.093474\n",
    "    df = df.groupby(by=['id', 'cell_id2'], as_index=False)['rank2'].agg('mean')\n",
    "        # df.groupby(by=['id', 'cell_id2'], as_index=True)['rank2'].agg('mean').head(10) =>\n",
    "        # id              cell_id2\n",
    "        # 0001bdd4021779  015d52a4    0.042571\n",
    "        #                 073782ca   -0.015153\n",
    "        #                 073e27e5   -0.021158\n",
    "        #                 07c52510   -0.010163\n",
    "        #                 0a1a7a39    0.006047\n",
    "        #                 0bcd3fef   -0.015030\n",
    "        #                 38310c80   -0.025221\n",
    "        #                 3fdc37be   -0.013544\n",
    "        #                 58bf360b    0.093474\n",
    "        #                 7fde4f04    0.083458\n",
    "        #                 80543cd8    0.004953\n",
    "        #                 8ea7263c   -0.016716\n",
    "        #                 ad7679ef    0.043002\n",
    "        # Name: rank2, dtype: float64    \n",
    "        \n",
    "        # ( showing all rows of id 0001bdd4021779 ), df.head(10) =>\n",
    "        #                 id  cell_id2     rank2\n",
    "        # 0   0001bdd4021779  015d52a4  0.042571\n",
    "        # 1   0001bdd4021779  073782ca -0.015153\n",
    "        # 2   0001bdd4021779  073e27e5 -0.021158\n",
    "        # 3   0001bdd4021779  07c52510 -0.010163\n",
    "        # 4   0001bdd4021779  0a1a7a39  0.006047\n",
    "        # 5   0001bdd4021779  0bcd3fef -0.015030\n",
    "        # 6   0001bdd4021779  38310c80 -0.025221\n",
    "        # 7   0001bdd4021779  3fdc37be -0.013544\n",
    "        # 8   0001bdd4021779  58bf360b  0.093474\n",
    "        # 9   0001bdd4021779  7fde4f04  0.083458\n",
    "        \n",
    "    df.rename(columns={'cell_id2': 'cell_id'}, inplace=True)\n",
    "        # code_df_valid.head(2) =>\n",
    "        #                id   cell_id     rank2\n",
    "        # 0  00001756c60be8  1862f0a6  0.033333\n",
    "        # 1  00001756c60be8  2a9e43d6  0.066667\n",
    "    code_df_valid_tmp = code_df_valid[code_df_valid['id'].isin(df['id'])]\n",
    "        # code_df_valid_tmp.head(2) =>    \n",
    "        #                  id   cell_id     rank2\n",
    "        # 102  0001bdd4021779  3fdc37be  0.090909\n",
    "        # 103  0001bdd4021779  073782ca  0.181818    \n",
    "    code_df_valid_tmp['rank3'] = code_df_valid_tmp.groupby(by=['id'])['rank2'].rank(ascending=True, method='first')\n",
    "        # code_df_valid_tmp.head(3) =>\n",
    "        #                  id   cell_id     rank2  rank3\n",
    "        # 102  0001bdd4021779  3fdc37be  0.090909    1.0\n",
    "        # 103  0001bdd4021779  073782ca  0.181818    2.0\n",
    "        # 104  0001bdd4021779  8ea7263c  0.272727    3.0\n",
    "        \n",
    "    # df is much larger dataframe than code_df_valid_tmp, thus we find many empty values in ['rank3'] -\n",
    "    # - column of \"code_df_valid_tmp\".\n",
    "    tmp = code_df_valid_tmp[['id', 'cell_id', 'rank3']].merge(df, how='inner', on=['id', 'cell_id'])\n",
    "        # tmp.head(3) =>    \n",
    "        #                id   cell_id  rank3     rank2\n",
    "        # 0  0001bdd4021779  3fdc37be    1.0 -0.013544\n",
    "        # 1  0001bdd4021779  073782ca    2.0 -0.015153\n",
    "        # 2  0001bdd4021779  8ea7263c    3.0 -0.016716    \n",
    "        \n",
    "    tmp['rank4'] = tmp.groupby(by=['id'])['rank2'].rank(ascending=True, method='first')\n",
    "        # tmp.head(15) =>    \n",
    "        #                 id   cell_id  rank3     rank2  rank4\n",
    "        # 0   0001bdd4021779  3fdc37be    1.0 -0.013544    6.0\n",
    "        # 1   0001bdd4021779  073782ca    2.0 -0.015153    4.0\n",
    "        # 2   0001bdd4021779  8ea7263c    3.0 -0.016716    3.0\n",
    "        # 3   0001bdd4021779  80543cd8    4.0  0.004953    8.0\n",
    "        # 4   0001bdd4021779  38310c80    5.0 -0.025221    1.0\n",
    "        # 5   0001bdd4021779  073e27e5    6.0 -0.021158    2.0\n",
    "        # 6   0001bdd4021779  015d52a4    7.0  0.042571   10.0\n",
    "        # 7   0001bdd4021779  ad7679ef    8.0  0.043002   11.0\n",
    "        # 8   0001bdd4021779  07c52510    9.0 -0.010163    7.0\n",
    "        # 9   0001bdd4021779  0a1a7a39   10.0  0.006047    9.0\n",
    "        # 10  0001bdd4021779  0bcd3fef   11.0 -0.015030    5.0\n",
    "        # 11  0002115f48f982  18281c6c    1.0 -0.024155    3.0\n",
    "        # 12  0002115f48f982  e3b6b115    2.0 -0.072884    2.0\n",
    "\n",
    "    tmp = tmp[['id', 'cell_id', 'rank3']].merge(tmp[['id', 'rank4', 'rank2']].rename(columns={'rank4': 'rank3'}),\n",
    "                                                how='inner', on=['id', 'rank3'])\n",
    "        # tmp.head(15) =>\n",
    "        #                 id   cell_id  rank3     rank2\n",
    "        # 0   0001bdd4021779  3fdc37be    1.0 -0.025221\n",
    "        # 1   0001bdd4021779  073782ca    2.0 -0.021158\n",
    "        # 2   0001bdd4021779  8ea7263c    3.0 -0.016716\n",
    "        # 3   0001bdd4021779  80543cd8    4.0 -0.015153\n",
    "        # 4   0001bdd4021779  38310c80    5.0 -0.015030\n",
    "        # 5   0001bdd4021779  073e27e5    6.0 -0.013544\n",
    "        # 6   0001bdd4021779  015d52a4    7.0 -0.010163\n",
    "        # 7   0001bdd4021779  ad7679ef    8.0  0.004953\n",
    "        # 8   0001bdd4021779  07c52510    9.0  0.006047\n",
    "        # 9   0001bdd4021779  0a1a7a39   10.0  0.042571\n",
    "        # 10  0001bdd4021779  0bcd3fef   11.0  0.043002\n",
    "        # 11  0002115f48f982  18281c6c    1.0 -0.125229\n",
    "        # 12  0002115f48f982  e3b6b115    2.0 -0.072884\n",
    "        # 13  0002115f48f982  4a044c54    3.0 -0.024155\n",
    "        # 14  0002115f48f982  365fe576    4.0 -0.014178\n",
    "        \n",
    "    tmp = tmp[['id', 'cell_id', 'rank2']]\n",
    "    df = df.merge(tmp[['id', 'cell_id', 'rank2']].rename(columns={'rank2': 'rank3'}), how='left', on=['id', 'cell_id'])\n",
    "    # now, ['rank3'] contains predictions for only code cells.\n",
    "    # ['rank2'] contains predictions for both cell types.\n",
    "        # df.head(4) =>\n",
    "        #                id   cell_id     rank2     rank3\n",
    "        # 0  0001bdd4021779  015d52a4  0.042571 -0.010163\n",
    "        # 1  0001bdd4021779  073782ca -0.015153 -0.021158\n",
    "        # 2  0001bdd4021779  073e27e5 -0.021158 -0.013544\n",
    "        # 3  0001bdd4021779  07c52510 -0.010163  0.006047        \n",
    "        \n",
    "    \n",
    "    df['rank2'] = np.where(pd.isnull(df['rank3']), df['rank2'], df['rank3']) \n",
    "\n",
    "    # df = pd.concat([df[['id', 'cell_id', 'rank2']], code_df_valid_tmp]).reset_index(drop=True)\n",
    "    \n",
    "    # sort ['id'] and then within each id sort ['rank2'].\n",
    "    df = df.sort_values(by=['id', 'rank2'], ascending=True)\n",
    "        # df.head(4) =>\n",
    "        #                 id   cell_id     rank2     rank3\n",
    "        # 7   0001bdd4021779  3fdc37be -0.025221 -0.025221\n",
    "        # 1   0001bdd4021779  073782ca -0.021158 -0.021158\n",
    "        # 11  0001bdd4021779  8ea7263c -0.016716 -0.016716\n",
    "        # 10  0001bdd4021779  80543cd8 -0.015153 -0.015153\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:01.389473Z",
     "iopub.status.busy": "2023-03-16T04:59:01.388729Z",
     "iopub.status.idle": "2023-03-16T04:59:01.400466Z",
     "shell.execute_reply": "2023-03-16T04:59:01.399714Z",
     "shell.execute_reply.started": "2023-03-16T04:59:01.389420Z"
    }
   },
   "outputs": [],
   "source": [
    "log_out_dir = os.path.join(parameter.result_dir, 'logs')\n",
    "os.makedirs(log_out_dir, exist_ok=True)\n",
    "log_dir = os.path.join(log_out_dir, '{}.txt'.format(args.model_name))\n",
    "if os.path.exists(log_dir):\n",
    "    os.remove(log_dir)\n",
    "log = utils.Logger()\n",
    "log.open(log_dir, mode='a')\n",
    "\n",
    "model_dir = os.path.join(parameter.result_dir, '')\n",
    "model_dir_1 = os.path.join(parameter.result_dir_1, '')\n",
    "# os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T04:59:44.890487Z",
     "iopub.status.busy": "2023-03-16T04:59:44.889527Z",
     "iopub.status.idle": "2023-03-16T04:59:44.895747Z",
     "shell.execute_reply": "2023-03-16T04:59:44.894701Z",
     "shell.execute_reply.started": "2023-03-16T04:59:44.890443Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-16T05:01:39.955289Z",
     "iopub.status.busy": "2023-03-16T05:01:39.954866Z",
     "iopub.status.idle": "2023-03-16T05:01:45.989451Z",
     "shell.execute_reply": "2023-03-16T05:01:45.988009Z",
     "shell.execute_reply.started": "2023-03-16T05:01:39.955251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 162.63it/s]\n",
      "../input/nar-ai4code-utils/preprocess.py:204: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['source'] = df['source'].str.replace(\"[SEP]\", \"\")\n",
      "../input/nar-ai4code-utils/preprocess.py:205: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['source'] = df['source'].str.replace(\"[CLS]\", \"\")\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Some weights of the model checkpoint at ../input/nar-transformers/deberta-v3-base/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# set random seeds\n",
    "seed_everything(parameter.random_seed)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "log.write('>> predicting...\\n')\n",
    "start = time.time()\n",
    "\n",
    "  \n",
    "test_df = preprocess.read_json_data(mode='test')\n",
    "test_df['rank'], test_df['fold_flag'] = 1,-1\n",
    "test_df = preprocess.preprocess_df(test_df)\n",
    "test_df = pd.concat(\n",
    "        [test_df[test_df['cell_type'] == 0], test_df[test_df['cell_type'] == 1].sample(frac=1.0)]).reset_index(\n",
    "        drop=True)\n",
    "test_df['rank2'] = (test_df.groupby(by=['id', 'cell_type']).cumcount() + 1) / \\\n",
    "                    test_df.groupby(by=['id', 'cell_type'])['cell_id'].transform('count')\n",
    "test_df.loc[test_df['cell_type'] == 1, 'rank2'] = -1\n",
    "code_df_sub = test_df[test_df['cell_type'] == 0][['id', 'cell_id', 'rank2']].copy()\n",
    "test_df = preprocess.get_truncated_df(test_df, cell_count=parameter.cell_count) \n",
    "\n",
    "# test_df = pd.read_parquet(\"../input/google-ai/train_df\")[:1000]\n",
    "# code_df_sub = pd.read_parquet(\"../input/google-ai/code_df_valid\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(utils.get_model_path(args.model_name, res = parameter.model_dir))\n",
    "test_df = get_sorted_test_df(test_df, 'source', tokenizer, batch_size=parameter.batch_size, cell_max_length=parameter.cell_max_length)\n",
    "\n",
    "test_pre = test_df[test_df['batch_max_length'] <= 4096]\n",
    "test_post = test_df[test_df['batch_max_length'] > 4096]\n",
    "\n",
    "model = MarkdownModel(utils.get_model_path(args.model_name, res = parameter.model_dir), pretrained=True)\n",
    "filename = os.path.join(model_dir_1, args.model_name + '_fold0.pth.tar')\n",
    "\n",
    "# -------------------- part1\n",
    "test_dataset = MarkdownDataset(test_pre, tokenizer, mode=\"test\", parameter=parameter)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=2,\n",
    "                         num_workers=args.workers, drop_last=False, pin_memory=True)\n",
    "y_preds, masks = get_preds(test_pre, test_loader, model, filename, max_length=4096+1024)\n",
    "\n",
    "\n",
    "# -------------------- part2\n",
    "if len(test_post) > 0:    \n",
    "    parameter.set(**{'seq_max_length': 4096+1024}) \n",
    "    test_dataset = MarkdownDataset(test_post, tokenizer, mode=\"test\", parameter=parameter)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1,\n",
    "                             num_workers=args.workers, drop_last=False, pin_memory=True)\n",
    "    y_preds2, masks2 = get_preds(test_post, test_loader, model, filename, max_length=4096+1024)\n",
    "    y_preds = np.concatenate([y_preds2, y_preds])\n",
    "    masks = np.concatenate([masks2, masks])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "res = get_score(test_df, masks, y_preds, code_df_sub)\n",
    "# res = pd.concat([res1, res2]).reset_index(drop=True)\n",
    "# sub_df = res1\n",
    "# sub_df = res1.sort_values(by=['id', 'rank2'], ascending=True)\n",
    "sub_df = res.groupby(by=['id'], sort=False)['cell_id'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "sub_df.rename(columns={'cell_id': 'cell_order'}, inplace=True)\n",
    "# sub_df['cell_order'] = sub_df['cell_order'].apply(lambda x: ' '.join(x.split()[::-1]))\n",
    "# print(test_df.shape)\n",
    "sub_df[['id', 'cell_order']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
