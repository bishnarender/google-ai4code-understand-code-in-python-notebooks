{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-23T15:13:19.561123Z",
     "iopub.status.busy": "2023-02-23T15:13:19.560115Z",
     "iopub.status.idle": "2023-02-23T15:13:19.580538Z",
     "shell.execute_reply": "2023-02-23T15:13:19.579380Z",
     "shell.execute_reply.started": "2023-02-23T15:13:19.561079Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "# from unidecode import unidecode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from https://www.kaggle.com/code/ilyaryabov/fastttext-sorting-with-cosine-distance-algo\n",
    "# import re\n",
    "# from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self):\n",
    "        # data\n",
    "        self.result_dir = './user_data/'\n",
    "        self.result_dir_1 = './kaggle/'\n",
    "        self.data_dir = '../input/AI4Code/'\n",
    "        self.model_dir = './models/'\n",
    "        self.k_folds = 5\n",
    "        self.random_seed = 27\n",
    "        self.seq_length = 512 # 512; sum of \"cell length\" for a particular id.\n",
    "        self.cell_max_length = 128 # max length of a cell.\n",
    "        self.cell_count = 128        \n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        # model\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.gpu = 0\n",
    "        self.print_freq = 500\n",
    "        self.weight_decay = 0\n",
    "        self.optim = 'Adam'\n",
    "\n",
    "    def get(self, name):\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def set(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(['%s:%s' % item for item in self.__dict__.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-23T15:13:17.000698Z",
     "iopub.status.busy": "2023-02-23T15:13:17.000214Z",
     "iopub.status.idle": "2023-02-23T15:13:17.017327Z",
     "shell.execute_reply": "2023-02-23T15:13:17.016103Z",
     "shell.execute_reply.started": "2023-02-23T15:13:17.000661Z"
    }
   },
   "outputs": [],
   "source": [
    "parameter = Parameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFold(object):\n",
    "    \"\"\"\n",
    "    KFold: Group split by group_col or random_split\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_seed, k_folds=10, flag_name='fold_flag'):\n",
    "        self.k_folds = k_folds\n",
    "        self.flag_name = flag_name\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    def group_split(self, train_df, group_col):\n",
    "        group_value = list(set(train_df[group_col]))\n",
    "            # type(group_value) => <class 'list'>\n",
    "            \n",
    "        # operations over list can be applied directly without assigning.\n",
    "        group_value.sort()        \n",
    "        fold_flag = [i % self.k_folds for i in range(len(group_value))]\n",
    "            # [i % 5 for i in range(50)]\n",
    "            # [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, ....]\n",
    "        np.random.shuffle(fold_flag)\n",
    "        \n",
    "        train_df = train_df.merge(pd.DataFrame({group_col: group_value, self.flag_name: fold_flag}), how='left',\n",
    "                                  on=group_col)\n",
    "        return train_df\n",
    "\n",
    "    def random_split(self, train_df):\n",
    "        fold_flag = [i % self.k_folds for i in range(len(train_df))]\n",
    "        np.random.shuffle(fold_flag)\n",
    "        train_df[self.flag_name] = fold_flag\n",
    "        return train_df\n",
    "\n",
    "    def stratified_split(self, train_df, group_col):\n",
    "        train_df[self.flag_name] = 1\n",
    "        train_df[self.flag_name] = train_df.groupby(by=[group_col])[self.flag_name].rank(ascending=True,\n",
    "                                                                                         method='first').astype(int)\n",
    "        train_df[self.flag_name] = train_df[self.flag_name].sample(frac=1.0).reset_index(drop=True)\n",
    "        train_df[self.flag_name] = train_df[self.flag_name] % self.k_folds\n",
    "        return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data(mode='train'):\n",
    "    paths_train = sorted(list(glob.glob(parameter.data_dir + '{}/*.json'.format(mode))))#[:100]\n",
    "    res = pd.concat([\n",
    "        pd.read_json(path, dtype={'cell_type': 'category', 'source': 'str'}).assign(\n",
    "            id=path.split('/')[-1].split('.')[0]).rename_axis('cell_id')\n",
    "            # .assign() => create new column ['id'] with values equal to file name after removing extension \".json\".\n",
    "            # .rename_axis() => provide name to index as ['cell_id'].\n",
    "        for path in tqdm(paths_train)]).reset_index(drop=False)\n",
    "            # .reset_index(drop=False) => converts the named index ['cell_id'] to column, then resets the index to numbering system.\n",
    "    res = res[['id', 'cell_id', 'cell_type', 'source']]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(document):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(document))\n",
    "    document = document.replace('_', ' ')\n",
    "\n",
    "    # matches any sequence of one or more whitespace characters followed by a single letter (uppercase or lowercase) followed by one or more whitespace characters.\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    #         # Remove single characters from the start\n",
    "    #         document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    # flags=re.I => argument specifies that the search should be case-insensitive.\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    #         # Removing prefixed 'b'\n",
    "    #         document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # return document\n",
    "\n",
    "    #         # Lemmatization\n",
    "    #         tokens = document.split()\n",
    "    #         tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    #         # tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "    #         preprocessed_text = ' '.join(tokens)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    # count ['cell_id'] under each under each ['id']    \n",
    "    df['cell_count'] = df.groupby(by=['id'])['cell_id'].transform('count')\n",
    "    \n",
    "    # df['source'] = df['cell_type'] + ' ' + df['source']\n",
    "    df['cell_type'] = df['cell_type'].map({'code': 0, 'markdown': 1}).fillna(0).astype(int)\n",
    "    # df.loc[df['cell_type']==0, 'source'] = df.loc[df['cell_type']==0, 'rank'] + ' ' + df.loc[df['cell_type']==0, 'source']\n",
    "    \n",
    "    # count markdown cell\n",
    "    df['markdown_count'] = df.groupby(by=['id'])['cell_type'].transform('sum')\n",
    "    \n",
    "    # count code cell\n",
    "    df['code_count'] = df['cell_count'] - df['markdown_count']\n",
    "    \n",
    "    # normalize rank by corresponding ['cell_count']\n",
    "    df['rank'] = df['rank'] / df['cell_count']\n",
    "    \n",
    "    # .strip() => leading and trailing whitespace removed\n",
    "    df['source'] = df['source'].apply(lambda x: x.lower().strip())\n",
    "    df['source'] = df['source'].apply(lambda x: preprocess_text(x))\n",
    "    # df['source'] = df['source'].replace(\"\\\\n\", \"\\n\")\n",
    "    # df['source'] = df['source'].str.replace(\"\\n\", \"\")\n",
    "    df['source'] = df['source'].str.replace(\"[SEP]\", \"\")\n",
    "    df['source'] = df['source'].str.replace(\"[CLS]\", \"\")\n",
    "\n",
    "    # df['source'] = df['source'].replace(\"#\", \"\")\n",
    "    # df['source'] = df['source'].apply(lambda x: unidecode(x))\n",
    "    \n",
    "    # replaces one or more consecutive spaces in the string x with a single space.    \n",
    "    df['source'] = df['source'].apply(lambda x: re.sub(' +', ' ', str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_truncated_df(df, cell_count=128, id_col='id2', group_col='id', max_random_cnt=500, expand_ratio=10):\n",
    "#     tmp1 = df[df['cell_count'] <= cell_count].reset_index(drop=True)\n",
    "#     tmp1.loc[:, id_col] = 0\n",
    "#     tmp2 = df[df['cell_count'] > cell_count].reset_index(drop=True)\n",
    "#     # print(tmp1.shape,tmp2.shape)\n",
    "#     res = [tmp1]\n",
    "#     for _, df_g in tmp2.groupby(by=group_col):\n",
    "#         # print(df_g.columns)\n",
    "#         df_g = df_g.sample(frac=1.0).reset_index(drop=True)\n",
    "#         # index_list = range(len(df_g))\n",
    "#         step = min(cell_count // 2, len(df_g) - cell_count)\n",
    "#         step = max(step, 1)\n",
    "#         id_col_count = 0\n",
    "#         for i in range(0, len(df_g), step):\n",
    "#             # indexes = [i] + list(np.random.choice([j for j in index_list if j!=i],cell_count-1, replace=False))\n",
    "#             # indexes = range(i,i+cell_count)\n",
    "#             # print(indexes,i,len(df_g),index_list)\n",
    "#             res_tmp = df_g.iloc[i:i + cell_count]  # .copy()\n",
    "#             # if len(res_tmp) == cell_count:\n",
    "#             res_tmp.loc[:, id_col] = id_col_count\n",
    "#             id_col_count += 1\n",
    "#             res.append(res_tmp)\n",
    "#\n",
    "#         random_cnt = int(len(df_g) // cell_count * expand_ratio)\n",
    "#         random_cnt = min(random_cnt, max_random_cnt)  # todo\n",
    "#         if random_cnt > 0:\n",
    "#             for i in range(random_cnt):\n",
    "#                 res_tmp = df_g.sample(n=cell_count).reset_index(drop=True)\n",
    "#                 res_tmp.loc[:, id_col] = id_col_count\n",
    "#                 id_col_count += 1\n",
    "#                 res.append(res_tmp)\n",
    "#\n",
    "#     res = pd.concat(res).reset_index(drop=True)\n",
    "#     sort_flag = range(len(res))\n",
    "#     np.random.shuffle(sort_flag)\n",
    "#     res.loc[res['cell_type'] == 0, 'sort_flag'] = 0\n",
    "#     res = res.sort_values(by=['id', id_col, 'cell_type', 'rank'], ascending=True)\n",
    "#     res = res.groupby(by=['id', id_col, 'fold_flag', 'cell_count'], as_index=False, sort=False)[\n",
    "#         ['cell_id', 'cell_type', 'source', 'rank']].agg(list)\n",
    "#     return res\n",
    "\n",
    "\n",
    "# divide id's (whose cell_count is greater than 128) into batches of \"size <= 128\", and then append to original df.\n",
    "def get_truncated_df(df, cell_count=128, id_col='id2', group_col='id', max_random_cnt=100, expand_ratio=5):\n",
    "    tmp1 = df[df['cell_count'] <= cell_count].reset_index(drop=True)\n",
    "    tmp1.loc[:, id_col] = 1\n",
    "    tmp2 = df[df['cell_count'] > cell_count].reset_index(drop=True)\n",
    "    # print(tmp1.shape,tmp2.shape)\n",
    "    res = [tmp1]\n",
    "    for _, df_g in tmp2.groupby(by=group_col):\n",
    "            # type(df_g) => <class 'pandas.core.frame.DataFrame'>\n",
    "            # df_g.columns => Index(['id', 'cell_id', 'cell_type', 'source', 'rank', 'ancestor_id',...\n",
    "            # cell_count => 128\n",
    "            \n",
    "        df_g = df_g.sample(frac=1.0).reset_index(drop=True)\n",
    "        step = min(cell_count // 2, len(df_g) - cell_count)\n",
    "        step = max(step, 1)\n",
    "            # cell_count // 2, len(df_g) => 64, 229\n",
    "            # len(df_g) - cell_count => 101\n",
    "            \n",
    "        id_col_count = 1\n",
    "        for i in range(0, len(df_g), step):# (0,229,64)\n",
    "            res_tmp = df_g.iloc[i:i + cell_count]  # .copy()\n",
    "            if len(res_tmp) != cell_count:\n",
    "                res_tmp = df_g.iloc[-cell_count:] # pick 128 rows from tail.\n",
    "            # if len(res_tmp) == cell_count:\n",
    "            res_tmp.loc[:, id_col] = id_col_count\n",
    "            id_col_count += 1\n",
    "            res.append(res_tmp)\n",
    "            if i + cell_count >= len(df_g):\n",
    "                break\n",
    "\n",
    "        if len(df_g) // cell_count > 1.3:# len(df_g) // cell_count => 1\n",
    "            random_cnt = int(len(df_g) // cell_count * expand_ratio)\n",
    "                # random_cnt = 10            \n",
    "            random_cnt = min(random_cnt, max_random_cnt)  # todo\n",
    "\n",
    "            for i in range(random_cnt):\n",
    "                res_tmp = df_g.sample(n=cell_count).reset_index(drop=True)\n",
    "                res_tmp.loc[:, id_col] = id_col_count\n",
    "                id_col_count += 1\n",
    "                res.append(res_tmp)\n",
    "\n",
    "    res = pd.concat(res).reset_index(drop=True)\n",
    "    res = res.sort_values(by=['id', id_col, 'cell_type', 'rank2'], ascending=True)\n",
    "    res = res.groupby(by=['id', id_col, 'fold_flag', 'cell_count', 'markdown_count', 'code_count'], as_index=False,\n",
    "                      sort=False)[\n",
    "        ['cell_id', 'cell_type', 'source', 'rank', 'rank2']].agg(list)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(seed=parameter.random_seed, mode=0):\n",
    "    if os.path.exists('./train_df.pkl'):\n",
    "        train_df = pd.read_pickle('./train_df.pkl')\n",
    "    else:\n",
    "        train_df = read_json_data(mode='train')\n",
    "        # train_orders; contains order of cells in a notebook/id.\n",
    "        train_orders = pd.read_csv('../input/AI4Code/' + 'train_orders.csv')\n",
    "        train_ancestors = pd.read_csv('../input/AI4Code/' + 'train_ancestors.csv')\n",
    "        \n",
    "        # converts 'cell_id's present in cell_order to list format.\n",
    "        train_orders['cell_id'] = train_orders['cell_order'].str.split()        \n",
    "\n",
    "        train_orders = train_orders.explode(column='cell_id')\n",
    "\n",
    "        train_orders['flag'] = range(len(train_orders))\n",
    "        train_orders['rank'] = train_orders.groupby(by=['id'])['flag'].rank(ascending=True, method='first').astype(int)\n",
    "            # method='first' => ranks assigned in order they appear in the array.\n",
    "        del train_orders['flag'], train_orders['cell_order']\n",
    "\n",
    "        # train_df = preprocess_features(train_df)\n",
    "        train_df = train_df.merge(train_orders, on=['id', 'cell_id'], how='left')\n",
    "        train_df = train_df.merge(train_ancestors[['id', 'ancestor_id']], on=['id'], how='left')\n",
    "        train_df.to_pickle('train_df.pkl')\n",
    "    \n",
    "    # create 'fold_flag' column over column 'ancestor_id'.\n",
    "    train_df = KFold(seed, parameter.k_folds).group_split(train_df, group_col='ancestor_id')    \n",
    "        # ancestor_id => ['0000585e', '00008df7', '0001831d', '00024696', '0002f785', '00045f09', '00055dd3', '00057f34', ... \n",
    "        # fold_flag => [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
    "    \n",
    "    # train_df = preprocess_features(train_df)\n",
    "    # train_df['source_length'] = train_df['source'].apply(len)\n",
    "    # train_df['id_length'] = train_df.groupby(by=['id'])['source_length'].transform('sum')\n",
    "    train_df = preprocess_df(train_df)\n",
    "    train_df = pd.concat(\n",
    "        [train_df[train_df['cell_type'] == 0], train_df[train_df['cell_type'] == 1].sample(frac=1.0)]).reset_index(\n",
    "        drop=True)\n",
    "        # frac=1.0 => 1.0 fraction of axis items to return. (1.0 means take all 100%).\n",
    "        # cell_type == 1 for markdown.\n",
    "        \n",
    "    # rank2; describes the order of cell for a particular ['cell_type'].\n",
    "    train_df['rank2'] = (train_df.groupby(by=['id', 'cell_type']).cumcount() + 1) / \\\n",
    "                        train_df.groupby(by=['id', 'cell_type'])['cell_id'].transform('count')\n",
    "    train_df.loc[train_df['cell_type'] == 1, 'rank2'] = -1\n",
    "    code_df_valid = train_df[train_df['cell_type'] == 0][['id', 'cell_id', 'rank2']].copy()\n",
    "\n",
    "    #     for col in ['cell_count','markdown_count', 'code_count']:\n",
    "    #         train_df[col] = (train_df[col] - train_df[col].mean())/ train_df[col].std()\n",
    "    #         train_df[col] = np.clip(train_df[col].fillna(0.0), -3, 3)\n",
    "\n",
    "\n",
    "    train_df = get_truncated_df(train_df, cell_count=parameter.cell_count)\n",
    "    #     train_df['flag'] = train_df['cell_type'].apply(lambda x:np.sum(x))\n",
    "    #     train_df = train_df[train_df['flag']>0]\n",
    "    #     del train_df['flag']\n",
    "    \n",
    "#     print(train_df)\n",
    "#     print(train_df.shape)\n",
    "    return train_df, code_df_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, code_df_valid = get_data()\n",
    "# train_df.to_parquet('train_df', compression = \"gzip\", index=False)\n",
    "# code_df_valid.to_parquet('code_df_valid', compression = \"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_parquet(\"train_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
